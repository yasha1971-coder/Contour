# auto_scan.py
# –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ACE + –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (10k —à–∞–≥–æ–≤) + –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ best_params.json

import os, json, itertools, subprocess, re, sys
from pathlib import Path

# -------- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Å–µ—Ç–∫–∞ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å) --------
GRID = {
    "NOISE":     [0.010, 0.012, 0.014],
    "MEM_DECAY": [0.34, 0.36, 0.38, 0.40],
    "HYST":      [0.0055, 0.0065, 0.0075],
}

# -------- –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–¥—Ä–∞ (–Ω–µ –ø–µ—Ä–µ—Ç–∏—Ä–∞—é—Ç—Å—è —Å–µ—Ç–∫–æ–π) --------
BASE = {
    "COUP_LA_TO_OM": 0.18,
    "COUP_OM_TO_LA": 0.20,
    "L_GAIN":        1.3,
    "VAR_WINDOW":    300,
}

# -------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ–∑–∞–ø—É—Å–∫–æ–≤ --------
SHORT_STEPS = "4000"
LONG_STEPS  = "10000"  # —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
RUNS_DIR    = Path("auto_runs")
RUNS_DIR.mkdir(exist_ok=True)

# -------- –†–∞–∑–±–æ—Ä –æ—Ç—á—ë—Ç–∞ --------
METRIC_PATTERNS = {
    "drift": r"Drift share:\s+([\d.]+)",
    "var":   r"var_win\(Œ©‚Ä≤\):\s+([\deE\+\-\.]+)",
    "mean":  r"mean \|dŒõ‚Ä≤/dt\|:\s+([\deE\+\-\.]+)",
}

def extract_metrics(text: str) -> dict:
    out = {}
    for k, pat in METRIC_PATTERNS.items():
        m = re.search(pat, text)
        out[k] = float(m.group(1)) if m else float("nan")
    out["verdict"] = "ALIVE" if "VERDICT: [ALIVE]" in text else "NOT ALIVE"
    return out

def score_metrics(m: dict) -> float:
    # –ß–µ–º –±–ª–∏–∂–µ mean –∫ 0.01 –∏ var –∫ 5e-4 ‚Äî —Ç–µ–º –ª—É—á—à–µ; ALIVE –¥–∞—ë—Ç –±–æ–Ω—É—Å
    if any((k not in m or (isinstance(m[k], float) and (m[k] != m[k]))) for k in ("drift","var","mean")):
        return -1.0
    score = 0.0
    if m["verdict"] == "ALIVE":
        score += 10.0
    score += min(m["drift"] / 50.0, 1.0)                    # 0..1
    score += (1.0 - min(abs(m["mean"] - 0.01)/0.01, 1.0))   # 0..1
    score += (1.0 - min(abs(m["var"]  - 5e-4)/5e-4, 1.0))   # 0..1
    return score

def run_once(params: dict, steps: str, out_dir: Path) -> str:
    out_dir.mkdir(parents=True, exist_ok=True)
    tmp_params = out_dir / "tmp_params.json"
    tmp_params.write_text(json.dumps(params, indent=2), encoding="utf-8")

    log_path = out_dir / "run.log"
    cmd = [
        sys.executable, "main.py",
        "--steps", steps,
        "--params", str(tmp_params),
        "--report-dir", str(out_dir),
    ]
    with log_path.open("w", encoding="utf-8") as f:
        subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT)

    return log_path.read_text(encoding="utf-8")

def main():
    summaries = []
    best_score = -1.0
    best = None

    # ---- –ü–∞—Ä—Å–∏–º —Å–µ—Ç–∫—É –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–≥–æ–Ω—ã ----
    for noise, mem, hyst in itertools.product(GRID["NOISE"], GRID["MEM_DECAY"], GRID["HYST"]):
        run_name = f"N{noise:.3f}_M{mem:.2f}_H{hyst:.4f}"
        params = {**BASE, "NOISE": noise, "MEM_DECAY": mem, "HYST": hyst}

        print(f"\n>>> Scan run: {run_name}")
        text = run_once(params, SHORT_STEPS, RUNS_DIR / run_name)
        m = extract_metrics(text)
        s = score_metrics(m)

        line = f"{run_name:28s} ‚Üí {m['verdict']:9s} | drift={m['drift']:5.1f}%  mean={m['mean']:.5f}  var={m['var']:.2e}  score={s:.3f}"
        print(line)
        summaries.append(line)

        if s > best_score:
            best_score = s
            best = {"params": params, "metrics": m, "score": s, "run_name": run_name}

    # ---- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ----
    (RUNS_DIR / "summary.txt").write_text("\n".join(summaries), encoding="utf-8")

    if not best:
        print("\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
        return

    # ---- –û–±–Ω–æ–≤–ª—è–µ–º best_params.json (–ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è) ----
    best_cfg = {**best["params"], **best["metrics"], "score": best["score"], "source_run": best["run_name"]}
    Path("best_params.json").write_text(json.dumps(best_cfg, indent=2), encoding="utf-8")
    print("\n‚úÖ Preliminary best configuration written to best_params.json")
    print(json.dumps(best_cfg, indent=2))

    # ---- –î–æ–ª–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (10k —à–∞–≥–æ–≤) ----
    print("\nüîÅ Stability check (10k steps)‚Ä¶")
    stable_dir = RUNS_DIR / f"{best['run_name']}_stability"
    text_long = run_once(best["params"], LONG_STEPS, stable_dir)
    m_long = extract_metrics(text_long)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    stability_report = (
        "===== STABILITY CHECK (10k) =====\n"
        f"Verdict: {m_long['verdict']}\n"
        f"Drift share: {m_long['drift']:.2f}%\n"
        f"mean |dŒõ‚Ä≤/dt|: {m_long['mean']:.5f}\n"
        f"var_win(Œ©‚Ä≤): {m_long['var']:.6e}\n"
    )
    (stable_dir / "stability_report.txt").write_text(stability_report, encoding="utf-8")
    print(stability_report)

    # ---- –ó–∞–∫—Ä–µ–ø–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –±–µ—Å—Ç, –µ—Å–ª–∏ ALIVE ----
    if m_long["verdict"] == "ALIVE":
        stable_best = {**best["params"], **m_long, "validated_on_steps": int(LONG_STEPS)}
        Path("stable_best_params.json").write_text(json.dumps(stable_best, indent=2), encoding="utf-8")
        print("üèÅ Stable ALIVE confirmed ‚Üí saved to stable_best_params.json")
    else:
        print("‚ö†Ô∏è Stability run NOT ALIVE. –ü—Ä–æ–≤–µ—Ä—å summary –∏ –ø–æ–¥–ø—Ä–∞–≤—å —Å–µ—Ç–∫—É/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

    print("\nAll runs completed. See:")
    print(f"  - {RUNS_DIR/'summary.txt'}  (–≤—Å–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–≥–æ–Ω—ã)")
    print(f"  - {stable_dir/'stability_report.txt'}  (–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ 10k)")
    print("  - best_params.json / stable_best_params.json")

if __name__ == "__main__":
    main()
